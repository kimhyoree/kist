{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\L3352\\.conda\\envs\\new2\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\L3352\\.conda\\envs\\new2\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "c:\\Users\\L3352\\.conda\\envs\\new2\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "#KIST과제1-3\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1,\n",
    "                               bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.inplanes = 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes =planes * block.expansion  # 이 부분이 빠졌었네요\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "          x = self.conv1(x)\n",
    "          x = self.bn1(x)\n",
    "          x = self.relu(x)\n",
    "          x = self.maxpool(x)\n",
    "\n",
    "          x = self.layer1(x)\n",
    "          x = self.layer2(x)\n",
    "          x = self.layer3(x)\n",
    "          x = self.layer4(x)\n",
    "\n",
    "          x = self.avgpool(x)\n",
    "          x = x.view(x.size(0), -1)\n",
    "          x = self.fc(x)\n",
    "\n",
    "          return x\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# 데이터 전처리\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),  # 이미지를 무작위로 자르고 224x224로 크기를 변경\n",
    "    transforms.RandomHorizontalFlip(),  # 50% 확률로 이미지를 수평으로 뒤집음\n",
    "    transforms.RandomRotation(10),      # -10에서 10도 사이의 무작위 회전\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# 데이터셋 및 데이터 로더 설정\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=100, shuffle=True, num_workers=2)\n",
    "testloader = DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 59\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[39mdel\u001b[39;00m inputs, labels, outputs\n\u001b[0;32m     57\u001b[0m scheduler\u001b[39m.\u001b[39mstep()  \u001b[39m# 스케줄러 스텝 업데이트\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m train_loss\u001b[39m.\u001b[39mappend(running_loss \u001b[39m/\u001b[39m (i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m))  \u001b[39m# train_loss에 추가\u001b[39;00m\n\u001b[0;32m     60\u001b[0m train_accuracies\u001b[39m.\u001b[39mappend(train_accuracy)  \u001b[39m# train_accuracy에 추가\u001b[39;00m\n\u001b[0;32m     62\u001b[0m train_losses\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39mmean(train_loss))  \u001b[39m# 에포크의 끝에서 train_losses에 추가\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_loss' is not defined"
     ]
    }
   ],
   "source": [
    "# ResNet 모델 인스턴스 생성\n",
    "net = ResNet(Bottleneck, [3, 4, 6, 3])\n",
    "net.to('cuda')\n",
    "\n",
    "# 손실 함수 및 옵티마이저 설정\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 모델 학습\n",
    "num_epochs = 10\n",
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "batch_sizes = [32, 64, 128]\n",
    "\n",
    "train_losses_all = []  # 모든 learning rate와 batch size에 대한 train loss를 저장\n",
    "test_losses_all = []  # 모든 learning rate와 batch size에 대한 test loss를 저장\n",
    "train_accuracies_all = []  # 모든 learning rate와 batch size에 대한 train accuracy를 저장\n",
    "test_accuracies_all = []  # 모든 learning rate와 batch size에 대한 test accuracy를 저장\n",
    "\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)  # 스케줄러 추가\n",
    "        trainloader = DataLoader(trainset, batch_size=bs, shuffle=True, num_workers=2)\n",
    "        testloader = DataLoader(testset, batch_size=bs, shuffle=False, num_workers=2)\n",
    "\n",
    "        train_losses = []   # Loss를 기록하기 위한 리스트\n",
    "        test_losses = []\n",
    "        train_accuracies = []  # 정확도를 기록하기 위한 리스트\n",
    "        test_accuracies = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            running_loss = 0.0\n",
    "            correct_train = 0\n",
    "            total_train = 0\n",
    "            net.train()\n",
    "\n",
    "            \n",
    "            \n",
    "            for i, data in enumerate(trainloader, 0):\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)   # 정확도 계산을 위한 부분\n",
    "                total_train += labels.size(0)\n",
    "                correct_train += (predicted == labels).sum().item()\n",
    "                train_accuracy = 100 * correct_train / total_train\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                del inputs, labels, outputs\n",
    "            scheduler.step()  # 스케줄러 스텝 업데이트\n",
    "\n",
    "            train_loss.append(running_loss / (i + 1))  # train_loss에 추가\n",
    "            train_accuracies.append(train_accuracy)  # train_accuracy에 추가\n",
    "\n",
    "            train_losses.append(np.mean(train_loss))  # 에포크의 끝에서 train_losses에 추가\n",
    "\n",
    "            running_loss = 0.0\n",
    "            correct_test = 0\n",
    "            total_test = 0\n",
    "            net.eval()\n",
    "\n",
    "            test_loss = []  # 에포크의 시작에서 test_loss 리스트 초기화\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for i, data in enumerate(testloader, 0):\n",
    "                    inputs, labels = data\n",
    "                    inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "\n",
    "                    outputs = net(inputs)\n",
    "                    _, predicted = torch.max(outputs.data, 1)   # 정확도 계산을 위한 부분\n",
    "                    total_test += labels.size(0)\n",
    "                    correct_test += (predicted == labels).sum().item()\n",
    "                    test_accuracy = 100 * correct_test / total_test\n",
    "\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    running_loss += loss.item()\n",
    "\n",
    "                    del inputs, labels, outputs\n",
    "                \n",
    "                test_loss.append(running_loss / (i + 1))  # test_loss에 추가\n",
    "                test_accuracies.append(test_accuracy)  # test_accuracy에 추가\n",
    "\n",
    "            test_losses.append(np.mean(test_loss))  # 에포크의 끝에서 test_losses에 추가\n",
    "\n",
    "        print(f\"Learning Rate: {lr}, Batch Size: {bs}, Train Loss: {train_losses[-1]:.4f}, Test Loss: {test_losses[-1]:.4f}, Train Accuracy: {train_accuracies[-1]:.2f}%, Test Accuracy: {test_accuracies[-1]:.2f}%\")\n",
    "        # 학습이 끝난 후 각 결과를 저장\n",
    "        train_losses_all.append(train_losses)\n",
    "        test_losses_all.append(test_losses)\n",
    "        train_accuracies_all.append(train_accuracies)\n",
    "        test_accuracies_all.append(test_accuracies)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Loss 그래프 그리기\n",
    "for i, params in enumerate(param_combinations):\n",
    "    lr, bs = params\n",
    "    plt.figure(i)\n",
    "    plt.plot(train_losses_all[i], label='Train Loss')\n",
    "    plt.plot(test_losses_all[i], label='Test Loss')\n",
    "    plt.title(f'Learning Rate: {lr}, Batch Size: {bs}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# 정확도 그래프 그리기\n",
    "for i, params in enumerate(param_combinations):\n",
    "    lr, bs = params\n",
    "    plt.figure(i)\n",
    "    plt.plot(train_accuracies_all[i], label='Train Accuracy')\n",
    "    plt.plot(test_accuracies_all[i], label='Test Accuracy')\n",
    "    plt.title(f'Learning Rate: {lr}, Batch Size: {bs}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
